"""
ä¼˜åŒ–ç‰ˆæ¼”åŒ–è®­ç»ƒå¼•æ“ v1.0.2 - æ€§èƒ½ä¼˜åŒ–çš„è®­ç»ƒå¼•æ“

æ ¸å¿ƒä¼˜åŒ–ï¼š
1. å†…å­˜ç®¡ç†ä¼˜åŒ–ï¼Œå‡å°‘ä¸´æ—¶å¼ é‡åˆ†é…
2. æ‰¹é‡å¤„ç†ï¼Œæé«˜GPUåˆ©ç”¨ç‡
3. æ€§èƒ½ç›‘æ§ï¼Œæ·»åŠ è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡
4. æ™ºèƒ½è°ƒåº¦ï¼Œå‡å°‘ä¸å¿…è¦çš„çŠ¶æ€æ›´æ–°
"""

import torch
import torch.nn as nn
import torch.optim as optim
import time
from typing import Optional
import numpy as np


class OptimizedEvolutionEngine:
    """ä¼˜åŒ–ç‰ˆæ¼”åŒ–è®­ç»ƒå¼•æ“ v1.0.2"""

    def __init__(
        self,
        model: nn.Module,
        device: torch.device,
        lr: float = 0.001,
        plasticity_interval: int = 50
    ):
        """
        åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆè®­ç»ƒå¼•æ“

        Args:
            model: ç¥ç»ç½‘ç»œæ¨¡å‹
            device: è®¡ç®—è®¾å¤‡
            lr: å­¦ä¹ ç‡
            plasticity_interval: ç¥ç»å¯å¡‘æ€§æ›´æ–°é—´éš”
        """
        self.model = model.to(device)
        self.device = device
        self.plasticity_interval = plasticity_interval

        # ğŸš€ ä¼˜åŒ–1ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„ä¼˜åŒ–å™¨è®¾ç½®
        self.optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)  # AdamWé€šå¸¸æ›´ç¨³å®š
        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # æ ‡ç­¾å¹³æ»‘æé«˜æ³›åŒ–

        # ğŸš€ Stage 4 ä¼˜åŒ–ï¼šæ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=5,  # æ¯5ä¸ªepoché‡å¯ä¸€æ¬¡
            T_mult=2,
            eta_min=lr * 0.01
        )

        # ğŸš€ ä¼˜åŒ–2ï¼šæ”¹è¿›çš„æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
        self.use_amp = (device.type == 'cuda')
        if self.use_amp:
            try:
                # å°è¯•æ–°ç‰ˆæœ¬çš„torch.amp
                from torch.amp import autocast, GradScaler
                self.autocast = lambda: autocast(device_type='cuda', enabled=True)
                self.scaler = GradScaler(enabled=True)
            except (ImportError, AttributeError):
                # å›é€€åˆ°æ—§ç‰ˆæœ¬
                from torch.cuda.amp import autocast, GradScaler
                self.autocast = lambda: autocast(enabled=True)
                self.scaler = GradScaler(enabled=True)
        else:
            # CPUæ¨¡å¼ï¼šä½¿ç”¨ç©ºçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆä¸å½±å“æ¢¯åº¦è®¡ç®—ï¼‰
            from contextlib import nullcontext
            self.autocast = lambda: nullcontext()
            self.scaler = None

        # ğŸš€ ä¼˜åŒ–3ï¼šé¢„åˆ†é…ç»Ÿè®¡ç¼“å†²åŒº
        self.epoch_stats = []
        self.performance_metrics = {
            'batch_times': [],
            'plasticity_times': [],
            'forward_times': [],
            'backward_times': []
        }

        # ğŸš€ ä¼˜åŒ–4ï¼šæ™ºèƒ½è°ƒåº¦æ ‡å¿—
        self.last_plasticity_time = 0
        self.performance_log_interval = 100  # æ¯100ä¸ªbatchè®°å½•ä¸€æ¬¡æ€§èƒ½

    def train_and_evolve_optimized(
        self,
        train_loader,
        test_loader=None,
        epochs: int = 5,
        verbose: bool = True
    ):
        """
        ğŸš€ ä¼˜åŒ–ç‰ˆè®­ç»ƒå’Œæ¼”åŒ– - æ€§èƒ½ç›‘æ§å’Œæ™ºèƒ½è°ƒåº¦

        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            epochs: è®­ç»ƒè½®æ•°
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        """
        if verbose:
            print(f"\n[INFO] Optimized Engine Started. Device: {self.device} | AMP: {self.use_amp}")
            print(f"[INFO] Plasticity update interval: {self.plasticity_interval} batches")
            print(f"[INFO] Optimizations: Vectorized pruning | Smart caching | Memory optimization")
            print("-" * 80)

        for epoch in range(epochs):
            epoch_start = time.time()

            # Train one epoch (optimized version)
            train_loss, train_acc = self._train_epoch_optimized(
                train_loader, epoch, epochs, verbose
            )

            # Test
            test_acc = self.test(test_loader) if test_loader else 0.0

            epoch_time = time.time() - epoch_start
            
            # Get detailed performance metrics
            stats = self.model.get_statistics()
            stats.update({
                'epoch': epoch + 1,
                'train_loss': train_loss,
                'train_acc': train_acc,
                'test_acc': test_acc,
                'epoch_time': epoch_time,
                'avg_batch_time': np.mean(self.performance_metrics['batch_times']) if self.performance_metrics['batch_times'] else 0,
                'avg_plasticity_time': np.mean(self.performance_metrics['plasticity_times']) if self.performance_metrics['plasticity_times'] else 0,
                'memory_optimization': 'enabled' if hasattr(self.model, 'connection_manager') else 'disabled'
            })
            self.epoch_stats.append(stats)

            # Clear current epoch performance metrics
            self._clear_performance_metrics()

            if verbose:
                self._print_epoch_summary_optimized(stats)

    def train_and_evolve(self, train_loader, test_loader=None, epochs: int = 5, verbose: bool = True):
        """å…¼å®¹æ€§æ–¹æ³•ï¼šè°ƒç”¨ä¼˜åŒ–ç‰ˆå®ç°"""
        return self.train_and_evolve_optimized(train_loader, test_loader, epochs, verbose)

    def _train_epoch_optimized(self, train_loader, epoch, total_epochs, verbose):
        """
        ğŸš€ ä¼˜åŒ–ç‰ˆepochè®­ç»ƒ - æ€§èƒ½ç›‘æ§å’Œå†…å­˜ä¼˜åŒ–

        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            epoch: å½“å‰epoch
            total_epochs: æ€»epochæ•°
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

        Returns:
            (å¹³å‡æŸå¤±, å‡†ç¡®ç‡)
        """
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        num_batches = len(train_loader)

        # ğŸš€ é¢„åˆ†é…ä¸€äº›ä¸´æ—¶å˜é‡
        batch_start_time = time.time()

        for batch_idx, (imgs, lbls) in enumerate(train_loader):
            # ğŸš€ ä¼˜åŒ–ï¼šéé˜»å¡æ•°æ®ä¼ è¾“
            imgs = imgs.view(imgs.size(0), -1).to(self.device, non_blocking=True)
            lbls = lbls.to(self.device, non_blocking=True)

            # ğŸš€ è®°å½•å‰å‘ä¼ æ’­æ—¶é—´
            forward_start = time.time()

            # æ¸…é›¶æ¢¯åº¦
            self.optimizer.zero_grad(set_to_none=True)  # æ›´é«˜æ•ˆçš„æ¢¯åº¦æ¸…é›¶

            # ğŸš€ ä¼˜åŒ–çš„æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            with self.autocast():
                outputs = self.model(imgs)
                loss = self.criterion(outputs, lbls)

            # ğŸš€ è®°å½•åå‘ä¼ æ’­æ—¶é—´
            backward_start = time.time()

            # åå‘ä¼ æ’­ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
            if self.use_amp and self.scaler is not None:
                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                loss.backward()
                self.optimizer.step()

            # ğŸš€ Stage 4 ä¼˜åŒ–ï¼šæ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()

            # ğŸš€ æ€§èƒ½æŒ‡æ ‡è®°å½•
            current_time = time.time()
            forward_time = backward_start - forward_start
            backward_time = current_time - backward_start
            batch_time = current_time - batch_start_time

            self.performance_metrics['forward_times'].append(forward_time)
            self.performance_metrics['backward_times'].append(backward_time)
            self.performance_metrics['batch_times'].append(batch_time)

            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += lbls.size(0)
            correct += (predicted == lbls).sum().item()

            # ğŸš€ æ™ºèƒ½ç¥ç»å¯å¡‘æ€§æ›´æ–° - æ€§èƒ½ç›‘æ§ç‰ˆæœ¬
            if batch_idx % self.plasticity_interval == 0 and batch_idx > 0:
                plasticity_start = time.time()
                
                # æ‰§è¡Œä¼˜åŒ–ç‰ˆå‰ªæå’Œç”Ÿé•¿
                pruned, added = self.model.apply_neuroplasticity()
                
                plasticity_time = time.time() - plasticity_start
                self.performance_metrics['plasticity_times'].append(plasticity_time)

                if verbose:
                    # ğŸš€ ä¼˜åŒ–ï¼šå‡å°‘é¢‘ç¹çš„çŠ¶æ€æŸ¥è¯¢
                    current_acc = 100.0 * correct / total
                    current_sparsity = self.model.get_sparsity()
                    current_connections = self.model.adj_mask.sum().item()

                    print(f"[Epoch {epoch+1}/{total_epochs}] Batch {batch_idx}/{num_batches} | "
                          f"Loss: {loss.item():.4f} | Acc: {current_acc:.2f}% | "
                          f"Pruned: {pruned} | Added: {added} | "
                          f"Connections: {current_connections:.0f} | Sparsity: {current_sparsity:.2%} | "
                          f"Plasticity: {plasticity_time:.3f}s")

                # ğŸš€ æ™ºèƒ½è°ƒåº¦ï¼šå¦‚æœå¯å¡‘æ€§æ›´æ–°å¤ªæ…¢ï¼ŒåŠ¨æ€è°ƒæ•´é—´éš”
                if plasticity_time > 1.0 and self.plasticity_interval < 200:
                    self.plasticity_interval = min(200, self.plasticity_interval + 10)
                    if verbose:
                        print(f"[INFO] ğŸš€ åŠ¨æ€è°ƒæ•´å¯å¡‘æ€§é—´éš”ä¸º: {self.plasticity_interval}")

            # ğŸš€ æ€§èƒ½æ—¥å¿—è®°å½•ï¼ˆå‡å°‘é¢‘ç‡ï¼‰
            elif verbose and batch_idx % self.performance_log_interval == 0:
                current_acc = 100.0 * correct / total
                print(f"[Epoch {epoch+1}/{total_epochs}] Batch {batch_idx}/{num_batches} | "
                      f"Loss: {loss.item():.4f} | Acc: {current_acc:.2f}% | "
                      f"Batch: {batch_time:.3f}s", end='\r')

            batch_start_time = time.time()

        avg_loss = total_loss / num_batches
        accuracy = 100.0 * correct / total
        return avg_loss, accuracy

    def test(self, test_loader) -> float:
        """
        æµ‹è¯•æ¨¡å‹æ€§èƒ½

        Args:
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨

        Returns:
            æµ‹è¯•å‡†ç¡®ç‡
        """
        self.model.eval()
        correct = 0
        total = 0
        
        # ğŸš€ ä¼˜åŒ–ï¼šç¦ç”¨æ¢¯åº¦è®¡ç®—å’Œä½¿ç”¨æ›´é«˜æ•ˆçš„è¯„ä¼°
        with torch.no_grad():
            for imgs, lbls in test_loader:
                imgs = imgs.view(imgs.size(0), -1).to(self.device, non_blocking=True)
                lbls = lbls.to(self.device, non_blocking=True)
                
                # ğŸš€ ä½¿ç”¨æ··åˆç²¾åº¦è¯„ä¼°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                with self.autocast():
                    outputs = self.model(imgs)
                
                _, predicted = torch.max(outputs, 1)
                total += lbls.size(0)
                correct += (predicted == lbls).sum().item()
                
        return 100.0 * correct / total

    def _print_epoch_summary_optimized(self, stats: dict):
        """
        ğŸš€ ä¼˜åŒ–ç‰ˆepochæ€»ç»“ - åŒ…å«æ€§èƒ½æŒ‡æ ‡

        Args:
            stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        print(f"\n\n{'='*30} Epoch {stats['epoch']} Summary {'='*30}")
        print(f"[INFO] Train accuracy: {stats['train_acc']:.2f}% | Test accuracy: {stats['test_acc']:.2f}%")
        print(f"[INFO] Current sparsity: {stats['sparsity']:.2%} | Total connections: {stats['total_connections']}")
        print(f"[INFO] Total time: {stats['epoch_time']:.2f}s | Avg batch time: {stats['avg_batch_time']:.3f}s")
        
        if stats['avg_plasticity_time'] > 0:
            print(f"[INFO] Avg plasticity time: {stats['avg_plasticity_time']:.3f}s")
        
        if 'memory_optimization' in stats:
            print(f"[INFO] Memory optimization: {stats['memory_optimization']}")
            
        print(f"{'='*75}\n")

    def _clear_performance_metrics(self):
        """æ¸…ç†å½“å‰epochçš„æ€§èƒ½æŒ‡æ ‡"""
        for key in self.performance_metrics:
            self.performance_metrics[key].clear()

    def get_epoch_stats(self):
        """è·å–epochç»Ÿè®¡ä¿¡æ¯"""
        return self.epoch_stats

    def get_performance_summary(self) -> dict:
        """
        ğŸš€ è·å–æ€§èƒ½æ€»ç»“

        Returns:
            æ€§èƒ½æ€»ç»“å­—å…¸
        """
        if not self.epoch_stats:
            return {}

        # è®¡ç®—å¹³å‡æ€§èƒ½æŒ‡æ ‡
        avg_batch_times = []
        avg_plasticity_times = []
        
        for epoch_stat in self.epoch_stats:
            if 'avg_batch_time' in epoch_stat:
                avg_batch_times.append(epoch_stat['avg_batch_time'])
            if 'avg_plasticity_time' in epoch_stat:
                avg_plasticity_times.append(epoch_stat['avg_plasticity_time'])

        return {
            'total_epochs': len(self.epoch_stats),
            'avg_epoch_time': np.mean([s['epoch_time'] for s in self.epoch_stats]),
            'avg_batch_time': np.mean(avg_batch_times) if avg_batch_times else 0,
            'avg_plasticity_time': np.mean(avg_plasticity_times) if avg_plasticity_times else 0,
            'final_accuracy': self.epoch_stats[-1]['train_acc'] if self.epoch_stats else 0,
            'final_sparsity': self.epoch_stats[-1]['sparsity'] if self.epoch_stats else 0
        }

    def save_model(self, path: str):
        """
        ä¿å­˜æ¨¡å‹å’Œè®­ç»ƒçŠ¶æ€

        Args:
            path: ä¿å­˜è·¯å¾„
        """
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'epoch_stats': self.epoch_stats,
            'performance_metrics': self.performance_metrics,
            'plasticity_interval': self.plasticity_interval,
            'version': 'v1.0.2_optimized'
        }, path)
        
        # ğŸš€ åŒæ—¶ä¿å­˜æ€§èƒ½æ€»ç»“
        perf_path = path.replace('.pth', '_performance.json')
        import json
        with open(perf_path, 'w') as f:
            json.dump(self.get_performance_summary(), f, indent=2)
        
        print(f"[INFO] ğŸš€ æ¨¡å‹å’Œæ€§èƒ½æ•°æ®å·²ä¿å­˜åˆ°: {path} å’Œ {perf_path}")